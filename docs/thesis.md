# Введение
Несмотря на рост числа киберугроз и общий уровень осведомленности пользователей,
ответственность за обеспечение безопасности по-прежнему в значительной степени
лежит на специалистах по информационной безопасности. Однако даже они не в
состоянии предусмотреть все возможные векторы атак. При этом существует группа
пользователей — разработчики, которые часто работают с недоверенным кодом и
обладают достаточными знаниями, чтобы, например, отключать некоторые меры защиты
(вроде запрета на копирование команд с sudo), но при этом могут не иметь
глубоких технических знаний в области безопасности. Для них особенно важно иметь
доступ к простым и эффективным средствам изоляции.

Однако существующие решения, такие как Docker, LXC, AppArmor, SELinux,
systemd-nspawn или Podman, изначально проектировались для профессионального
использования — в серверных или корпоративных инфраструктурах. Они предполагают
наличие административных прав, знание внутреннего устройства Linux и часто
требуют предварительной настройки. Более того, многие из них ориентированы не на
запуск отдельных процессов, а на полноценную оркестрацию окружений: с настройкой
томов, сетей, ресурсов, зависимостей, политик безопасности и доступа. В
результате попытка использовать такие инструменты для простой задачи — например,
разовое исполнение небольшого скрипта, скопированного с форума или
сгенерированного нейросетью — приводит к тому, что пользователь либо тратит
неоправданно много времени на настройку, либо отказывается от идеи изоляции
вовсе. Так как он вряд ли будет готов тратить время на чтение документации и
написание конфигурации ради скрипта, который, как ему кажется, будет работать
пару секунд. В результате вопрос изоляции и безопасности откладывается «на
потом» — до первого инцидента.

А так как проблема безопасного исполнения недоверенного кода сегодня стоит не
только перед специалистами в области информационной безопасности, но и перед
широкой аудиторией пользователей, которые регулярно сталкиваются с
необходимостью запускать потенциально опасные программы, то ее решение требует
инструментов новой направленности — достаточно надёжных, чтобы обеспечить
защиту, и достаточно простых, чтобы их было удобно использовать по принципу
«одна команда — и системе ничего не угрожает» (в большинстве случаев).

Целью данной работы является создание удобного в использовании инструмента,
позволяющего запускать недоверенный код в изолированной среде без необходимости
в привилегированных операциях и сложной конфигурации.

Для достижения поставленной цели необходимо решить следующие задачи:
1. Проанализировать существующие инструменты контейнеризации и их ограничения.
1. Определить ключевые требования к программному решению с учетом безопасности и
   удобства использования.
1. Разработать архитектуру программного решения.
1. Реализовать прототип программного решения.
1. Провести тестирование прототипа на соответствие требуемым критериям.


# Обзор существующих решений
Для формирования целостного представления о современных инструментах для
изоляции был проведен первичный сбор данных из нескольких авторитетных
источников. Основой для начального анализа послужили следующие ресурсы:

- [awesome-linux-containers](https://github.com/Friz-zy/awesome-linux-containers)
  — A curated list of awesome Linux Containers frameworks, libraries and
  software
- [awesome-containers](https://github.com/pditommaso/awesome-containers) — A
  curated list of awesome Linux containers related technologies inspired by
  other Awesome lists
- [Open Repository for Container Tools](https://github.com/containers/) — A
  collection of open source tools that create, configure, and work with
  containers

На основании информации из указанных источников была составлена обобщенная
таблица, включающая свыше **восьмидесяти** инструментов, связанных с
изолированным исполнением кода. Для сужения круга анализа использовались три
критерия (на момент 20.04.2025):
1. Популярность проекта: инструмент должен иметь не менее 1к звёзд (stars) на
   GitHub или аналогичной метрикой с другого хостинга — данный показатель, хоть
   и не является абсолютной метрикой качества, отражает уровень внимания
   сообщества.
1. Актуальность разработки: проект не должен находиться в архиве и иметь
   коммиты, слитые PR или решенные issues в течение последнего года — это
   исключает заброшенные и потенциально небезопасные решения.
1. Иметь открытый исходный код - почему.

После фильтрации осталась выборка из инструментов, активно поддерживаемых и
востребованных сообществом, которая представлена в таблице ниже:

| Название   | Звезд | Активность | Описание |
|------------|-------|------------|----------|
| **docker** | 69600 | 2025-04-20 |          |

Из данного списка были отобраны только те инструменты, которые предназначены для
создания изолированных сред исполнения. В него не вошли средства,
специализирующиеся, например, исключительно на мониторинге, аудите, анализе
образов или отладке контейнеров.

Отобранные решения можно классифицировать по назначению и архитектурным
особенностям на четыре основные группы:
1. Контейнерные менеджеры — комплексные системы для управления окружениями;
1. Инструменты с повышенной изоляцией — решения, использующие виртуализацию,
   эмуляцию системных вызовов или другие подходы для усиления безопасности;
1. Средства пользовательской изоляции — утилиты, позволяющие запускать процессы
   в контролируемой среде без дополнительных привилегий;
1. OCI-совместимые рантаймы — низкоуровневые компоненты, обеспечивающие
   исполнение контейнеров в соответствии с Open Container Initiative. Далее
   рассмотрим каждую группу подробнее.

## Контейнерные менеджеры
К этой категории относятся инструменты, которые предоставляют пользователю
полный цикл работы с изолированными окружениями: от создания контейнера до его
запуска, конфигурации, обновления и удаления. Это универсальные решения,
сочетающие в себе механизмы изоляции, управление файловыми системами, настройку
сетей, взаимодействие с реестрами образов и, зачастую, поддержку оркестрации. В
большинстве случаев они ориентированы на производственные среды: серверные,
облачные и корпоративные инфраструктуры, где важны масштабируемость, мониторинг,
сетевые политики и безопасность.

### Docker
Docker был разработан в 2013 году компанией dotCloud (впоследствии
переименованной в Docker Inc.) и за считанные годы стал одной из самых
узнаваемых технологий в мире разработки и эксплуатации программного обеспечения.
Он появился в тот момент, когда индустрия остро нуждалась в стандартизированном
способе упаковывать приложения вместе со всеми зависимостями, не полагаясь на
особенности окружения, в котором они будут запущены. Уже к 2015 году Docker
приобрел массовую популярность, а его экосистема начала стремительно расти:
появились реестры образов, инструменты оркестрации и полноценные
DevOps-платформы, основанные на контейнерах.

Тем не менее, при всех своих достоинствах, Docker не всегда подходит для других
задач — в частности, для изоляции недоверенного кода в пользовательских
сценариях. Архитектура Docker построена вокруг отдельного фонового демона
dockerd, который запускается с привилегиями суперпользователя и управляет всем
процессом запуска контейнеров. Это означает, что даже если команда запускается
от имени обычного пользователя, фактическое выполнение происходит с повышенными
правами, через взаимодействие с демоном.

Кроме того, запуск Docker-контейнера требует установки и настройки самого
Docker, что само по себе может быть нетривиальной задачей, особенно в
дистрибутивах, не имеющих полной поддержки по умолчанию. Пользователю нужно не
только понимать архитектуру образов и слоёв, но и быть готовым к созданию
Dockerfile, настройки сети, томов и прочих параметров. Это резко усложняет его
использование в сценариях, где требуется просто и быстро изолировать выполнение
одного-единственного скрипта. Особенно это чувствуется в пользовательских средах
без прав администратора, где установка Docker попросту невозможна без
вмешательства в системную конфигурацию.

Таким образом Docker оказывается не самым подходящим инструментом для цели,
которая была поставлена в данной работе, безопасного и изолированного выполнения
недоверенного кода в пользовательской среде. Его архитектурные особенности,
требования к привилегиям и относительная тяжеловесность делают его избыточным в
ситуациях, где главную роль играют простота, скорость и отсутствие необходимости
в настройке.

### LXC и LXD
LXC (Linux Containers) появился в 2008 году как первая попытка собрать воедино
все возможности изоляции, которые к тому времени уже были в ядре Linux. Он не
требует гипервизора и позволяет запускать процессы прямо на хост-системе, но
так, будто они работают в собственной, полностью отдельной среде. Это делает LXC
лёгким и быстрым по сравнению с классической виртуализацией — никакой эмуляции
железа, минимум накладных расходов, всё работает почти напрямую.

Сильные стороны LXC чувствуются сразу при первоначальном изучении: это
инструмент, который даёт разработчику или системному инженеру практически полный
контроль. Настроить пользовательские идентификаторы, cgroups, capabilities — всё
доступно и подробно настраивается. Но именно эта свобода и становится
опасностью. Без хорошего понимания устройства Linux быстро наткнешься на
тонкости с правами, безопасностью и взаимодействием между контейнером и хостом.

Кроме того, LXC больше ориентирован на долгоживущие контейнеры — с полноценной
init-системой, постоянным состоянием и отдельной жизнью. Для задач временной
изоляции, например, одноразового запуска подозрительного скрипта, его
использование оказывается слишком сложным, тяжеловесным и избыточным.

Со временем, чтобы упростить работу с LXC, появилась надстройка LXD — более
высокоуровневый инструмент, добавляющий REST API, автоматическую конфигурацию,
управление снапшотами и удобную командную строку. При этом сама архитектура
осталась прежней: LXC и LXD позволяют запускать полноценные «системные»
контейнеры, поведение которых близко к виртуальным машинам. Это особенно удобно
для тестирования, развертывания инфраструктуры или изоляции служб в рамках
одного хоста. Что, к сожалению, не является целью данной работы. Таким образом
LXD не является аналогом, а имеет другой вектор направленности.

### Podman
Podman появился в 2018 году как попытка переосмыслить контейнеризацию с учётом
накопленного опыта и слабых мест Docker. Разработанный Red Hat, он с самого
начала стремился решить два главных недостатка предшественника: необходимость
фонового демона и запуск контейнеров с привилегиями.

На первый взгляд, Podman почти неотличим от Docker — команды те же, поведение
схожее, образы совместимы. Но за внешней схожестью скрывается иная архитектура.
Вместо единого демона, который требует root-доступ и централизованно управляет
контейнерами, Podman запускает каждый контейнер как обычный процесс, от имени
пользователя. Это простое на первый взгляд решение меняет очень многое: оно
устраняет потребность в лишнем фоновом сервисе, уменьшает поверхность атаки и
делает систему более предсказуемой.

Особенно хорошо Podman показывает себя в больших системах: там, где важна
безопасность по умолчанию, где root-доступ ограничен, где контейнеры живут долго
и интегрируются в более сложные цепочки — CI/CD, автоматизация, управляемая
инфраструктура.

Тем не менее, несмотря на более современную архитектуру, Podman остаётся прежде
всего инструментом для разработчиков, инженеров и тех, кто регулярно работает с
контейнерами. Для обычного пользователя, перед которым стоит задача запустить
потенциально опасный скрипт в изолированной среде, Podman всё ещё может
показаться громоздким. Нужно установить зависимости, разобраться с тем, как
именно запускаются rootless-контейнеры, как работают user namespaces, как
правильно монтировать директории и т. д. То есть, даже без демона, это всё ещё
контейнерная платформа — мощная, гибкая, но не мгновенно понятная.

## Инструменты с повышенной безопасностью
В неё попадают решения, в которых изоляция и защита явно приоритетнее
производительности, удобства или совместимости. Они создавались не столько ради
запуска абстрактного «контейнера», сколько ради четкой цели: сделать так, чтобы
изолированный процесс точно не повлиял на систему, даже если он полностью
скомпрометирован.

**gVisor**, разработанный Google, — это не контейнерная система в привычном
смысле, а скорее слой между приложением и ядром Linux. Он реализует собственную
«виртуальную» реализацию системных вызовов, фактически создавая второе,
минимальное ядро, работающее в пространстве пользователя. Процессы не получают
прямого доступа к системным вызовам хоста, все обращения к ядру фильтруются и
обрабатываются gVisor-ом. Такая модель изоляции обеспечивает гораздо более
жёсткий контроль и серьезно снижает вероятность выхода из контейнера, но и дает
о себе знать — она медленнее, требовательнее, сложнее в отладке. Особенно это
чувствуется при запуске чего-то не совсем стандартного или ресурсоемкого. Тем не
менее, для запуска подозрительного кода, где риск важнее скорости, gVisor
оказывается вполне подходящим решением.

**Firecracker**, в свою очередь, — продукт Amazon, созданный специально для
запуска микро виртуальных машин (микро-ВМ) в облаке. В отличие от gVisor, он не
эмулирует ядро, а запускает полноценные виртуальные машины поверх KVM, но делает
это настолько быстро и экономно, что граница между виртуализацией и
контейнеризацией почти стирается. Каждая такая микро-ВМ изолирована аппаратно,
загружается за миллисекунды и почти не требует ресурсов. На выходе — уровень
защиты, сравнимый с полноценной виртуалкой, но со скоростью и лёгкостью
контейнеров. Подход впечатляющий, но и требующий. Для использования Firecracker
нужны права администратора, явное управление образами ядра и root-файловой
системой, понимание архитектуры виртуализации. В повседневной разработке он
скорее избыточен, но для инфраструктуры — идеален.

Таким образом, и gVisor, и Firecracker представляют собой шаг в сторону от
классических контейнеров: они сложнее, но и безопаснее. Если задача требует не
просто изоляции, а доверия к этой изоляции — они одни из лучших кандидатов. Но
обычному пользователю, которому просто нужно «чтобы скрипт не тронул систему»,
они могут показаться чересчур громоздкими и требовательными.

## Контейнерные рантаймы низкого уровня
Контейнерные рантаймы низкого уровня — это инструменты, которые обычно остаются
вне поля зрения, но именно они запускают контейнер. Когда мы используем знакомые
утилиты вроде Docker или Podman, всё в итоге сводится к вызову конкретного
рантайма, который берет на себя всю низкоуровневую работу: создает пространства
имён, настраивает ограничения, монтирует нужные директории и запускает процесс.
Большинство пользователей напрямую с ними не взаимодействуют, но от выбора
рантайма зависит, насколько быстро и безопасно будет работать контейнер.

Первым и до сих пор самым распространённым таким инструментом стал **runc**. Он
был создан как часть Docker, а позже стал отдельным проектом под управлением
Open Container Initiative. Его главная ценность — это стандартизованность.
Практически любая платформа, поддерживающая OCI, совместима с runc.

*crun*

*youki*

Таким образом, все три рантайма решают одну и ту же задачу, но с разным
подходом: runc — стабильный, crun — быстрый, youki — безопасный. Однако они
остаются компонентами инфраструктуры — их использование предполагает, что поверх
есть некий управляющий слой. В отрыве от таких систем они пригодны скорее для
интеграции, чем для самостоятельного применения обычными пользователями.

## Инструменты пользовательской изоляции
Одним из заметных решений в области пользовательской изоляции является
**udocker**. Его основная цель — предоставить возможность запуска контейнеров на
системах, где установка Docker невозможна или нежелательна, в первую очередь — в
пользовательском пространстве, без необходимости обладать административными
правами.

*nsjail*

*firejail*

*apparmor*

*bubblewrap*


# Обзор механизмов контейнеризации

## Namespaces
Механизм пространств имён (namespaces) в Linux позволяет изолировать различные
аспекты среды выполнения процессов. С его помощью можно добиться того, чтобы
процессы «не видели» остальную часть системы: они получают ограниченный обзор —
свой собственный PID-пространство, файловую систему, сеть, список пользователей
и т. д. (будет рассмотрено позднее) На практике это означает, что процесс может
считать, что он — единственный на системе, тогда как на самом деле изолирован
лишь его контекст.

Пространства имён стали основой для большинства современных решений по
контейнеризации, так как позволяют задать минимальный уровень изоляции без
обращения к полноценной виртуализации. При этом они не обеспечивают полной
безопасности — лишь создают границы, которые можно использовать в сочетании с
другими механизмами контроля.

## cgroups

## seccomp

## selinux

# Linux namespaces
Linux namespaces — это один из ключевых механизмов, лежащих в основе
контейнеризации. Именно благодаря ним процессы в изолированных окружениях могут
«не видеть» друг друга, будто работают на разных машинах, хотя физически
находятся на одном ядре. Это не какая-то новая надстройка, а часть ядра Linux,
развивающаяся уже более двух десятилетий.

Первые шаги в сторону изоляции в Linux начались еще в начале 2000-х годов, но в
полноценную систему пространства имён оформились с появлением CLONE_NEWNS в ядре
2.4.19 (2002 год), а позже начали появляться и остальные. Их развитие шло
постепенно: один namespace — одна изолированная часть системы. Сейчас существует
семь основных пространств имён, каждый из которых отвечает за свою область
изоляции: процессы, пользователи, сеть, монтирование, IPC, hostname и контроль
групп. Всё вместе это позволяет запускать процесс в «песочнице», где у него свои
PID'ы, свои смонтированные директории, своя сеть и даже свой root-пользователь,
не совпадающий с root'ом на хосте.

Интересный исторический момент связан с тем, почему именно namespace
монтирования в системных вызовах обозначается как CLONE_NEWNS, а не, скажем,
CLONE_NEWMOUNT. Всё дело в том, что он был первым реализованным пространством
имен в ядре Linux. На тот момент ещё не существовало общей концепции
"namespaces" как набора взаимосвязанных механизмов — был просто механизм
изоляции точек монтирования, и сокращённое newns (от new namespace) казалось
вполне логичным. Уже позже, когда начали появляться другие типы пространств
имён, их стали именовать более явно: CLONE_NEWPID, CLONE_NEWNET, CLONE_NEWUSER и
так далее.

Основная идея namespaces — дать процессу иллюзию того, что он находится в
собственной системе. При этом, в отличие от виртуальных машин, здесь нет полной
эмуляции: ядро одно, ресурсы общие, а изоляция реализована средствами самого
ядра, что делает такую модель намного более легкой и быстрой.

Благодаря этому namespaces стали основой для таких инструментов, как Docker,
LXC, Podman, Firejail, Bubblewrap и многих других. Они позволяют запускать
процессы в контейнерах без виртуализации и без необходимости поднимать отдельную
ОС. Более того, namespaces можно использовать напрямую — например, с помощью
unshare или более удобных оберток.

## User
User namespace — один из ключевых механизмов изоляции в Linux, появившийся в
ядре 3.8. Он позволяет процессам внутри изолированной среды иметь другие
идентификаторы пользователя и групп, чем на хосте. Благодаря этому обычный
пользователь может запустить процесс с правами root внутри контейнера, не
получая настоящего root-доступа на системе. Другими словами, он позволяет
запускать «привилегированные» процессы в изолированной среде, где эти привилегии
действуют только внутри и не имеют силы за её пределами.

Эта особенность открыла дорогу так называемым rootless-контейнерам —
контейнерам, которые можно запускать без административных прав. Именно user
namespace делает возможной изоляцию, не требующую вмешательства системного
администратора, что особенно важно в пользовательских и десктопных сценариях.

На практике он часто используется вместе с другими пространствами имён,
поскольку доступ к ним (например, к монтированию файловых систем или созданию
собственных PID-деревьев) тоже требует user namespace.

## Mount
Mount namespace даёт каждому контейнеру своё собственное представление о том,
как устроена файловая система. Это позволяет перенастроить точки монтирования
так, чтобы процессы внутри видели только то, что им разрешено, и в том виде, в
каком это задумал автор окружения. Директории можно подменить, сделать
доступными только для чтения, заменить на временные или вообще исключить — всё
это будет касаться только контейнера, а хост же останется нетронутым. Один и тот
же файл но в разных пространствах имен может оказаться в совершенно разных
местах, с разными правами и даже под разными именами.

В Linux, где почти всё — это файл: устройства, процессы, сокеты, настройки —
возможность контролировать, какие именно файлы видны и как они доступны,
оказывается критически важной. Неудивительно, что именно mount namespace
появился первым: он дает очень точный и мощный инструмент управления средой
исполнения.

## UTS
UTS namespace отвечает за изоляцию идентификаторов хоста — имени машины
(hostname) и домена (domainname). Это может показаться несущественным по
сравнению с пространствами имён, которые изолируют процессы или файловую
систему, но на практике дает важную гибкость. Это удобно и для организации
логов, и для отладки, и просто для визуального разделения сред.

## PID
PID namespace изолирует процессы: внутри него каждый запущенный процесс получает
собственный идентификатор, независимый от остальной системы. Это значит, что
контейнер не только не будет иметь доступа к процессам за пределами своего
пространства имен, но и также даже не знать о их существовании. Особенно важно
то, что внутри такого namespace можно создать собственное дерево процессов с
отдельным init-процессом (обычно с PID 1), который будет выполнять те же
функции, что и в полноценной системе — обрабатывать сигналы, управлять жизненным
циклом дочерних процессов и так далее. Это приближает поведение контейнера к
виртуальной машине, но без всей тяжести гипервизоров.

## Network
Network namespace позволяет каждому контейнеру иметь собственный сетевой стек.
Это значит, что у него будут свои интерфейсы, таблицы маршрутизации, правила
iptables, сокеты и даже отдельный экземпляр loopback-интерфейса (lo). В
результате контейнер можно отключить от сети вовсе, подключить к виртуальному
мосту, настроить NAT или дать прямой доступ к физическому интерфейсу — всё это
делается независимо от основной системы. Такой подход особенно удобен для
сценариев, где важно контролировать, с кем и как общается запущенный процесс.
Например, можно запустить подозрительное приложение с полной изоляцией от
интернета, сохранив при этом доступ к локальным файлам. Или наоборот — дать
выход в сеть, но запретить доступ к внутренним сервисам хоста

## IPC
IPC namespace отвечает за изоляцию механизмов межпроцессного взаимодействия —
таких как очереди сообщений, семафоры и общая память. Без него процессы внутри
разных контейнеров могут влиять друг друга, использовать одни и те же участки
памяти или вмешиваться в чужие взаимодействия. Это может привести не только к
ошибкам, но и к серьезным уязвимостям.

Когда используется отдельный IPC namespace, каждый контейнер получает
собственное пространство для таких объектов, полностью отделенное от остальной
системы. Это особенно важно, если в контейнере работает что-то чувствительное к
совместному доступу — например, PostgreSQL или Redis.
