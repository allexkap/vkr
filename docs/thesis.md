# Введение
Несмотря на рост числа киберугроз и общий уровень осведомленности пользователей,
ответственность за обеспечение безопасности по-прежнему в значительной степени
лежит на специалистах по информационной безопасности. Однако даже они не в
состоянии предусмотреть все возможные векторы атак. При этом существует группа
пользователей — разработчики, которые часто работают с недоверенным кодом и
обладают достаточными знаниями, чтобы, например, отключать некоторые меры защиты
(вроде запрета на копирование команд с sudo), но при этом могут не иметь
глубоких технических знаний в области безопасности. Для них особенно важно иметь
доступ к простым и эффективным средствам изоляции.

Однако существующие решения, такие как Docker, LXC, AppArmor, SELinux,
systemd-nspawn или Podman, изначально проектировались для профессионального
использования — в серверных или корпоративных инфраструктурах. Они предполагают
наличие административных прав, знание внутреннего устройства Linux и часто
требуют предварительной настройки. Более того, многие из них ориентированы не на
запуск отдельных процессов, а на полноценную оркестрацию окружений: с настройкой
томов, сетей, ресурсов, зависимостей, политик безопасности и доступа. В
результате попытка использовать такие инструменты для простой задачи — например,
разовое исполнение небольшого скрипта, скопированного с форума или
сгенерированного нейросетью — приводит к тому, что пользователь либо тратит
неоправданно много времени на настройку, либо отказывается от идеи изоляции
вовсе. Так как он вряд ли будет готов тратить время на чтение документации и
написание конфигурации ради скрипта, который, как ему кажется, будет работать
пару секунд. В результате вопрос изоляции и безопасности откладывается «на
потом» — до первого инцидента.

А так как проблема безопасного исполнения недоверенного кода сегодня стоит не
только перед специалистами в области информационной безопасности, но и перед
широкой аудиторией пользователей, которые регулярно сталкиваются с
необходимостью запускать потенциально опасные программы, то ее решение требует
инструментов новой направленности — достаточно надёжных, чтобы обеспечить
защиту, и достаточно простых, чтобы их было удобно использовать по принципу
«одна команда — и системе ничего не угрожает» (в большинстве случаев).

Целью данной работы является создание удобного в использовании инструмента,
позволяющего запускать недоверенный код в изолированной среде без необходимости
в привилегированных операциях и сложной конфигурации.

Для достижения поставленной цели необходимо решить следующие задачи:
1. Проанализировать существующие инструменты контейнеризации и их ограничения.
1. Определить ключевые требования к программному решению с учетом безопасности и
   удобства использования.
1. Разработать архитектуру программного решения.
1. Реализовать прототип программного решения.
1. Провести тестирование прототипа на соответствие требуемым критериям.


# Обзор существующих решений
Для формирования целостного представления о современных инструментах для
изоляции был проведен первичный сбор данных из нескольких авторитетных
источников. Основой для начального анализа послужили следующие ресурсы:

- [awesome-linux-containers](https://github.com/Friz-zy/awesome-linux-containers)
  — A curated list of awesome Linux Containers frameworks, libraries and
  software
- [awesome-containers](https://github.com/pditommaso/awesome-containers) — A
  curated list of awesome Linux containers related technologies inspired by
  other Awesome lists
- [Open Repository for Container Tools](https://github.com/containers/) — A
  collection of open source tools that create, configure, and work with
  containers

На основании информации из указанных источников была составлена обобщенная
таблица, включающая свыше **восьмидесяти** инструментов, связанных с
изолированным исполнением кода. Для сужения круга анализа использовались три
критерия (на момент 20.04.2025):
1. Популярность проекта: инструмент должен иметь не менее 1к звёзд (stars) на
   GitHub или аналогичной метрикой с другого хостинга — данный показатель, хоть
   и не является абсолютной метрикой качества, отражает уровень внимания
   сообщества.
1. Актуальность разработки: проект не должен находиться в архиве и иметь
   коммиты, слитые PR или решенные issues в течение последнего года — это
   исключает заброшенные и потенциально небезопасные решения.
1. Иметь открытый исходный код - почему.

После фильтрации осталась выборка из инструментов, активно поддерживаемых и
востребованных сообществом, которая представлена в таблице ниже:

| Название   | Звезд | Активность | Описание |
|------------|-------|------------|----------|
| **docker** | 69600 | 2025-04-20 |          |

Из данного списка были отобраны только те инструменты, которые предназначены для
создания изолированных сред исполнения. В него не вошли средства,
специализирующиеся, например, исключительно на мониторинге, аудите, анализе
образов или отладке контейнеров.

Отобранные решения можно классифицировать по назначению и архитектурным
особенностям на четыре основные группы:
1. Контейнерные менеджеры — комплексные системы для управления окружениями;
1. Инструменты с повышенной изоляцией — решения, использующие виртуализацию,
   эмуляцию системных вызовов или другие подходы для усиления безопасности;
1. Средства пользовательской изоляции — утилиты, позволяющие запускать процессы
   в контролируемой среде без дополнительных привилегий;
1. OCI-совместимые рантаймы — низкоуровневые компоненты, обеспечивающие
   исполнение контейнеров в соответствии с Open Container Initiative. Далее
   рассмотрим каждую группу подробнее.

## Контейнерные менеджеры
К этой категории относятся инструменты, которые предоставляют пользователю
полный цикл работы с изолированными окружениями: от создания контейнера до его
запуска, конфигурации, обновления и удаления. Это универсальные решения,
сочетающие в себе механизмы изоляции, управление файловыми системами, настройку
сетей, взаимодействие с реестрами образов и, зачастую, поддержку оркестрации. В
большинстве случаев они ориентированы на производственные среды: серверные,
облачные и корпоративные инфраструктуры, где важны масштабируемость, мониторинг,
сетевые политики и безопасность.

### Docker
Docker был разработан в 2013 году компанией dotCloud (впоследствии
переименованной в Docker Inc.) и за считанные годы стал одной из самых
узнаваемых технологий в мире разработки и эксплуатации программного обеспечения.
Он появился в тот момент, когда индустрия остро нуждалась в стандартизированном
способе упаковывать приложения вместе со всеми зависимостями, не полагаясь на
особенности окружения, в котором они будут запущены. Уже к 2015 году Docker
приобрел массовую популярность, а его экосистема начала стремительно расти:
появились реестры образов, инструменты оркестрации и полноценные
DevOps-платформы, основанные на контейнерах.

Тем не менее, при всех своих достоинствах, Docker не всегда подходит для других
задач — в частности, для изоляции недоверенного кода в пользовательских
сценариях. Архитектура Docker построена вокруг отдельного фонового демона
dockerd, который запускается с привилегиями суперпользователя и управляет всем
процессом запуска контейнеров. Это означает, что даже если команда запускается
от имени обычного пользователя, фактическое выполнение происходит с повышенными
правами, через взаимодействие с демоном.

Кроме того, запуск Docker-контейнера требует установки и настройки самого
Docker, что само по себе может быть нетривиальной задачей, особенно в
дистрибутивах, не имеющих полной поддержки по умолчанию. Пользователю нужно не
только понимать архитектуру образов и слоёв, но и быть готовым к созданию
Dockerfile, настройки сети, томов и прочих параметров. Это резко усложняет его
использование в сценариях, где требуется просто и быстро изолировать выполнение
одного-единственного скрипта. Особенно это чувствуется в пользовательских средах
без прав администратора, где установка Docker попросту невозможна без
вмешательства в системную конфигурацию.

Таким образом Docker оказывается не самым подходящим инструментом для цели,
которая была поставлена в данной работе, безопасного и изолированного выполнения
недоверенного кода в пользовательской среде.

### LXC и LXD
LXC (Linux Containers) появился в 2008 году как первая попытка собрать воедино
все возможности изоляции, которые к тому времени уже были в ядре Linux. Он не
требует гипервизора и позволяет запускать процессы прямо на хост-системе, но
так, будто они работают в собственной, полностью отдельной среде. Это делает LXC
лёгким и быстрым по сравнению с классической виртуализацией — никакой эмуляции
железа, минимум накладных расходов, всё работает почти напрямую.

Сильные стороны LXC чувствуются сразу при первоначальном изучении: это
инструмент, который даёт разработчику или системному инженеру практически полный
контроль. Настроить пользовательские идентификаторы, cgroups, capabilities — всё
доступно и подробно настраивается. Но именно эта свобода и становится
опасностью. Без хорошего понимания устройства Linux быстро наткнешься на
тонкости с правами, безопасностью и взаимодействием между контейнером и хостом.

Кроме того, LXC больше ориентирован на долгоживущие контейнеры — с полноценной
init-системой, постоянным состоянием и отдельной жизнью. Для задач временной
изоляции, например, одноразового запуска подозрительного скрипта, его
использование оказывается слишком сложным, тяжеловесным и избыточным.

Со временем, чтобы упростить работу с LXC, появилась надстройка LXD — более
высокоуровневый инструмент, добавляющий REST API, автоматическую конфигурацию,
управление снапшотами и удобную командную строку. При этом сама архитектура
осталась прежней: LXC и LXD позволяют запускать полноценные «системные»
контейнеры, поведение которых близко к виртуальным машинам. Это особенно удобно
для тестирования, развертывания инфраструктуры или изоляции служб в рамках
одного хоста. Что, к сожалению, не является целью данной работы. Таким образом
LXD не является аналогом, а имеет другой вектор направленности.

### Podman
Podman появился в 2018 году как попытка переосмыслить контейнеризацию с учётом
накопленного опыта и слабых мест Docker. Разработанный Red Hat, он с самого
начала стремился решить два главных недостатка предшественника: необходимость
фонового демона и запуск контейнеров с привилегиями.

На первый взгляд, Podman почти неотличим от Docker — команды те же, поведение
схожее, образы совместимы. Но за внешней схожестью скрывается иная архитектура.
Вместо единого демона, который требует root-доступ и централизованно управляет
контейнерами, Podman запускает каждый контейнер как обычный процесс, от имени
пользователя. Это простое на первый взгляд решение меняет очень многое: оно
устраняет потребность в лишнем фоновом сервисе, уменьшает поверхность атаки и
делает систему более предсказуемой.

Особенно хорошо Podman показывает себя в больших системах: там, где важна
безопасность по умолчанию, где root-доступ ограничен, где контейнеры живут долго
и интегрируются в более сложные цепочки — CI/CD, автоматизация, управляемая
инфраструктура.

Тем не менее, несмотря на более современную архитектуру, Podman остаётся прежде
всего инструментом для разработчиков, инженеров и тех, кто регулярно работает с
контейнерами. Для обычного пользователя, перед которым стоит задача запустить
потенциально опасный скрипт в изолированной среде, Podman всё ещё может
показаться громоздким. Нужно установить зависимости, разобраться с тем, как
именно запускаются rootless-контейнеры, как работают user namespaces, как
правильно монтировать директории и т. д. То есть, даже без демона, это всё ещё
контейнерная платформа — мощная, гибкая, но не мгновенно понятная.

## Инструменты с повышенной безопасностью
В неё попадают решения, в которых изоляция и защита явно приоритетнее
производительности, удобства или совместимости. Они создавались не столько ради
запуска абстрактного «контейнера», сколько ради четкой цели: сделать так, чтобы
изолированный процесс точно не повлиял на систему, даже если он полностью
скомпрометирован.

**gVisor**, разработанный Google, — это не контейнерная система в привычном
смысле, а скорее слой между приложением и ядром Linux. Он реализует собственную
«виртуальную» реализацию системных вызовов, фактически создавая второе,
минимальное ядро, работающее в пространстве пользователя. Процессы не получают
прямого доступа к системным вызовам хоста, все обращения к ядру фильтруются и
обрабатываются gVisor-ом. Такая модель изоляции обеспечивает гораздо более
жёсткий контроль и серьезно снижает вероятность выхода из контейнера, но и дает
о себе знать — она медленнее, требовательнее, сложнее в отладке. Особенно это
чувствуется при запуске чего-то не совсем стандартного или ресурсоемкого. Тем не
менее, для запуска подозрительного кода, где риск важнее скорости, gVisor
оказывается вполне подходящим решением.

**Firecracker**, в свою очередь, — продукт Amazon, созданный специально для
запуска микро виртуальных машин (микро-ВМ) в облаке. В отличие от gVisor, он не
эмулирует ядро, а запускает полноценные виртуальные машины поверх KVM, но делает
это настолько быстро и экономно, что граница между виртуализацией и
контейнеризацией почти стирается. Каждая такая микро-ВМ изолирована аппаратно,
загружается за миллисекунды и почти не требует ресурсов. На выходе — уровень
защиты, сравнимый с полноценной виртуалкой, но со скоростью и лёгкостью
контейнеров. Подход впечатляющий, но и требующий. Для использования Firecracker
нужны права администратора, явное управление образами ядра и root-файловой
системой, понимание архитектуры виртуализации. В повседневной разработке он
скорее избыточен, но для инфраструктуры — идеален.

Таким образом, и gVisor, и Firecracker представляют собой шаг в сторону от
классических контейнеров: они сложнее, но и безопаснее. Если задача требует не
просто изоляции, а доверия к этой изоляции — они одни из лучших кандидатов. Но
обычному пользователю, которому просто нужно «чтобы скрипт не тронул систему»,
они могут показаться чересчур громоздкими и требовательными.

## Контейнерные рантаймы низкого уровня
Контейнерные рантаймы низкого уровня — это инструменты, которые обычно остаются
вне поля зрения, но именно они запускают контейнер. Когда мы используем знакомые
утилиты вроде Docker или Podman, всё в итоге сводится к вызову конкретного
рантайма, который берет на себя всю низкоуровневую работу: создает пространства
имён, настраивает ограничения, монтирует нужные директории и запускает процесс.
Большинство пользователей напрямую с ними не взаимодействуют, но от выбора
рантайма зависит, насколько быстро и безопасно будет работать контейнер.

Первым и до сих пор самым распространённым таким инструментом стал **runc**. Он
был создан как часть Docker, а позже стал отдельным проектом под управлением
Open Container Initiative. Его главная ценность — это стандартизованность.
Практически любая платформа, поддерживающая OCI, совместима с runc.

*crun*

*youki*

Таким образом, все три рантайма решают одну и ту же задачу, но с разным
подходом: runc — стабильный, crun — быстрый, youki — безопасный. Однако они
остаются компонентами инфраструктуры — их использование предполагает, что поверх
есть некий управляющий слой. В отрыве от таких систем они пригодны скорее для
интеграции, чем для самостоятельного применения обычными пользователями.

## Инструменты пользовательской изоляции
Одним из заметных решений в области пользовательской изоляции является
**udocker**. Его основная цель — предоставить возможность запуска контейнеров на
системах, где установка Docker невозможна или нежелательна, в первую очередь — в
пользовательском пространстве, без необходимости обладать административными
правами.

*nsjail*

*firejail*

*apparmor*

*bubblewrap*


# Обзор механизмов контейнеризации
Контейнеризация в Linux — это не отдельная технология, а набор возможностей,
встроенных в ядро. Они решают разные задачи, но собираются вместе, когда нужно
ограничить процесс: в доступе к системе, в использовании ресурсов, в поведении.
Изначально эти механизмы создавались для других целей — например, учёт ресурсов
в многопользовательских системах или изоляция сетевых стеков — но со временем
оказалось, что они хорошо сочетаются. И можно обойтись без виртуальной машины,
если достаточно просто задать жесткие рамки.

Не существует одного способа собрать «правильный» контейнер. Все зависит от
того, что именно нужно ограничить, насколько серьезна угроза, и кто в итоге
будет пользоваться системой. Где-то важнее производительность, где-то контроль.
Поэтому прежде чем что-то собирать, стоит понять, из чего вообще можно строить.

## Namespaces
Если нужно, чтобы запущенный процесс не взаимодействовал напрямую с остальной
системой, первым шагом почти всегда становится использование пространств имён.
Они позволяют создать копии определенных частей окружения: таблицы процессов,
структуры сети, точек монтирования, идентификаторов пользователей.

Такой подход уменьшает количество точек взаимодействия с остальной системой.
Внутри namespace процесс может считать себя единственным, даже если на самом
деле это не так. Изоляция достигается на уровне видимости, а не прав. Эта идея
оказалась настолько гибкой, что на неё теперь опирается почти всё, что связано с
контейнерами, так как позволяют задать минимальный уровень изоляции без
обращения к полноценной виртуализации.

Данный механизм детальнее будет рассмотрен позднее.

## cgroups
Когда среда изолирована, остаётся вопрос — сколько ресурсов ей можно дать. Без
ограничений один процесс может занять всю память или загрузить процессор до
предела. Контроль за этим берут на себя cgroups (control group). Они разбивают
доступные ресурсы на группы и распределяют их между процессами. И делают это на
уровне ядра, что гарантирует предсказуемость: даже если процесс пытается обойти
ограничения, они всё равно срабатывают.

Поддерживаются ограничения на CPU, память, ввод-вывод и другие подсистемы.
Вторая версия cgroups упростила конфигурацию и сделала поведение более
стабильным, но общая логика осталась прежней.

## seccomp
Ограничить видимость и ресурсы — этого недостаточно, если процесс может делать
произвольные системные вызовы. Чаще всего это mount, ptrace, execve и другие
критичные точки, через которые можно повлиять на окружение. Именно тут вступает
в силу Seccomp (secure computing mode). Он даёт способ задать фильтр: какие
вызовы разрешены, а какие нет. Всё, что выходит за пределы, блокируется. Это
снижает риск даже в случае, если остальная изоляция оказалась неэффективной. Так
как даже если процесс получил доступ к какому-то ресурсу, без системного вызова
он не сможет им воспользоваться.

## selinux
SELinux добавляет ещё один уровень ограничений, основанный не на изоляции или
ресурсах, а на политике взаимодействия между компонентами. Это система, которая
описывает, кто с кем и на каких условиях может взаимодействовать. Даже если
процесс имеет права и работает в своём пространстве имён, SELinux может
запретить ему доступ к конкретному ресурсу, если это не предусмотрено в
политике. Это делает поведение системы более строгим, но и более предсказуемым.

Такой подход может казаться избыточным, но для чувствительных систем — например,
в промышленной инфраструктуре или в банковском секторе — это способ быть
уверенным, что никто не выйдет за пределы заранее согласованных сценариев.
SELinux требует аккуратной настройки: его политика (или отсутствие нужной
политики) может легко заблокировать требуемое поведение.


# Linux namespaces
Linux namespaces — это один из ключевых механизмов, лежащих в основе
контейнеризации. Именно благодаря ним процессы в изолированных окружениях могут
«не видеть» друг друга, будто работают на разных машинах, хотя физически
находятся на одном ядре. Это не какая-то новая надстройка, а часть ядра Linux,
развивающаяся уже более двух десятилетий.

Первые шаги в сторону изоляции в Linux начались еще в начале 2000-х годов, но в
полноценную систему пространства имён оформились с появлением CLONE_NEWNS в ядре
2.4.19 (2002 год), а позже начали появляться и остальные. Их развитие шло
постепенно: один namespace — одна изолированная часть системы. Сейчас существует
семь основных пространств имён, каждый из которых отвечает за свою область
изоляции: процессы, пользователи, сеть, монтирование, IPC, hostname и контроль
групп. Всё вместе это позволяет запускать процесс в «песочнице», где у него свои
PID'ы, свои смонтированные директории, своя сеть и даже свой root-пользователь,
не совпадающий с root'ом на хосте.

Интересный исторический момент связан с тем, почему именно namespace
монтирования в системных вызовах обозначается как CLONE_NEWNS, а не, скажем,
CLONE_NEWMOUNT. Всё дело в том, что он был первым реализованным пространством
имен в ядре Linux. На тот момент ещё не существовало общей концепции
"namespaces" как набора взаимосвязанных механизмов — был просто механизм
изоляции точек монтирования, и сокращённое newns (от new namespace) казалось
вполне логичным. Уже позже, когда начали появляться другие типы пространств
имён, их стали именовать более явно: CLONE_NEWPID, CLONE_NEWNET, CLONE_NEWUSER и
так далее.

Основная идея namespaces — дать процессу иллюзию того, что он находится в
собственной системе. При этом, в отличие от виртуальных машин, здесь нет полной
эмуляции: ядро одно, ресурсы общие, а изоляция реализована средствами самого
ядра, что делает такую модель намного более легкой и быстрой.

Благодаря этому namespaces стали основой для таких инструментов, как Docker,
LXC, Podman, Firejail, Bubblewrap и многих других. Они позволяют запускать
процессы в контейнерах без виртуализации и без необходимости поднимать отдельную
ОС. Более того, namespaces можно использовать напрямую — например, с помощью
unshare или более удобных оберток.

## User
User namespace — один из ключевых механизмов изоляции в Linux, появившийся в
ядре 3.8. Он позволяет процессам внутри изолированной среды иметь другие
идентификаторы пользователя и групп, чем на хосте. Благодаря этому обычный
пользователь может запустить процесс с правами root внутри контейнера, не
получая настоящего root-доступа на системе. Другими словами, он позволяет
запускать «привилегированные» процессы в изолированной среде, где эти привилегии
действуют только внутри и не имеют силы за её пределами.

Эта особенность открыла дорогу так называемым rootless-контейнерам —
контейнерам, которые можно запускать без административных прав. Именно user
namespace делает возможной изоляцию, не требующую вмешательства системного
администратора, что особенно важно в пользовательских и десктопных сценариях.

На практике он часто используется вместе с другими пространствами имён,
поскольку доступ к ним (например, к монтированию файловых систем или созданию
собственных PID-деревьев) тоже требует user namespace.

## Mount
Mount namespace даёт каждому контейнеру своё собственное представление о том,
как устроена файловая система. Это позволяет перенастроить точки монтирования
так, чтобы процессы внутри видели только то, что им разрешено, и в том виде, в
каком это задумал автор окружения. Директории можно подменить, сделать
доступными только для чтения, заменить на временные или вообще исключить — всё
это будет касаться только контейнера, а хост же останется нетронутым. Один и тот
же файл но в разных пространствах имен может оказаться в совершенно разных
местах, с разными правами и даже под разными именами.

В Linux, где почти всё — это файл: устройства, процессы, сокеты, настройки —
возможность контролировать, какие именно файлы видны и как они доступны,
оказывается критически важной. Неудивительно, что именно mount namespace
появился первым: он дает очень точный и мощный инструмент управления средой
исполнения.

## UTS
UTS namespace отвечает за изоляцию идентификаторов хоста — имени машины
(hostname) и домена (domainname). Это может показаться несущественным по
сравнению с пространствами имён, которые изолируют процессы или файловую
систему, но на практике дает важную гибкость. Это удобно и для организации
логов, и для отладки, и просто для визуального разделения сред.

## PID
PID namespace изолирует процессы: внутри него каждый запущенный процесс получает
собственный идентификатор, независимый от остальной системы. Это значит, что
контейнер не только не будет иметь доступа к процессам за пределами своего
пространства имен, но и также даже не знать о их существовании.

Особенно важно то, что внутри такого namespace можно создать собственное дерево
процессов с отдельным init-процессом (обычно с PID 1), который будет выполнять
те же функции, что и в полноценной системе — обрабатывать сигналы, управлять
жизненным циклом дочерних процессов и так далее. Это приближает поведение
контейнера к виртуальной машине, но без всей тяжести гипервизоров.

## Network
Network namespace позволяет каждому контейнеру иметь собственный сетевой стек.
Это значит, что у него будут свои интерфейсы, таблицы маршрутизации, правила
iptables, сокеты и даже отдельный экземпляр loopback-интерфейса (lo). В
результате контейнер можно отключить от сети вовсе, подключить к виртуальному
мосту, настроить NAT или дать прямой доступ к физическому интерфейсу — всё это
делается независимо от основной системы.

Такой подход особенно удобен для сценариев, где важно контролировать, с кем и
как общается запущенный процесс. Например, можно запустить подозрительное
приложение с полной изоляцией от интернета, сохранив при этом доступ к локальным
файлам. Или наоборот — дать выход в сеть, но запретить доступ к внутренним
сервисам хоста

## IPC
IPC namespace отвечает за изоляцию механизмов межпроцессного взаимодействия —
таких как очереди сообщений, семафоры и общая память. Без него процессы внутри
разных контейнеров могут влиять друг друга, использовать одни и те же участки
памяти или вмешиваться в чужие взаимодействия. Это может привести не только к
ошибкам, но и к серьезным уязвимостям.

Когда используется отдельный IPC namespace, каждый контейнер получает
собственное пространство для таких объектов, полностью отделенное от остальной
системы. Это особенно важно, если в контейнере работает что-то чувствительное к
совместному доступу — например, PostgreSQL или Redis.


# Проектирование архитектуры
*Тут краткое описание главы*

## Выбор инструментов изоляции
*Тут краткое описание раздела*

В первую очередь внимание падает на LXC. Это зрелое решение, позволяющее
разворачивать контейнеры, близкие по поведению к полноценным системам. У него
широкие возможности: отдельная сеть, снапшоты, интеграция с init-системой. Но
всё это сказывается на использовании — LXC рассчитан на долгоживущие контейнеры,
где важна устойчивость и контроль. В контексте кратковременных изолированных
процессов — это уже лишнее.

AppArmor использует другой подход. Это не контейнер, не песочница, а скорее
фильтр — он описывает, к каким ресурсам может обращаться программа. Это мощный
механизм, особенно в сочетании с остальной политикой безопасности системы. Но
для динамического запуска или гибкой настройки под каждый новый скрипт он
подходит плохо. Профили создаются отдельно, требуют установки с правами
администратора и не меняются на лету. Его применение имеет смысл тогда, когда
известно поведение программы и оно не будет меняться.

Всё это подводит к bubblewrap. Этот инструмент выделяется на фоне остальных
своей минималистичной моделью: никаких конфигов, никаких профилей, только
командная строка, в которой напрямую указывается, какие изоляции применить. Он
создавался как часть Flatpak и ориентирован на запуск приложений от обычного
пользователя, без привлечения root. Это делает его особенно подходящим для
задач, где хочется жестко ограничить процесс, но не вовлекать лишнюю сложность.
Пространства имён, tmpfs, readonly точки монтирования , ограничение доступа —
всё явно прописывается при запуске.

## Выбор языка программирования
Когда базовая технология изоляции определена — в данном случае это Bubblewrap —
остаётся решить, каким образом строить вокруг нее удобную оболочку. Сам
Bubblewrap — это утилита командной строки, и взаимодействие с ней всегда
сводится к вызову процесса с набором параметров. Подход прямой, но быстро
становится громоздким: количество флагов растёт, логика усложняется, а
необходимость реагировать на разные сценарии требует хоть какой-то структуры.

На этом этапе становится ясно, что нужен язык, в котором можно было бы быстро
собрать утилиту, обрабатывающую аргументы, управляя логикой подготовки окружения
и запуском. Python оказался уместным выбором. Он легко читается, стандартная
библиотека закрывает почти все базовые потребности — от работы с путями и
файлами до запуска процессов и логирования. Никакой дополнительной сборки или
компиляции не требуется, зависимости минимальны, запуск возможен на большинстве
современных Linux-систем без установки дополнительных компонентов, Python чаще
всего входит в стандартную поставку.

Это не самый производительный инструмент, но в этой задаче и не требуется
высокая производительность, так как это не CPU-bound задача. Гораздо важнее
простота изменений и прозрачность логики.

### argparse
argparse — стандартный инструмент для обработки аргументов командной строки.
Главное удобство в том, что логика определения и логика разбора аргументов
объединяются в одном месте: один раз описываются флаги, типы значений, что
является обязательным, а что — опциональным. Дальше argparse сам разбирает всё
на нужные элементы, проверяет корректность, выдает ошибки при неверном вводе и
даже генерирует справку, которую не нужно писать отдельно, что намного удобнее
чем обрабатывать sys.argv вручную. Такой подход не только делает код короче, но
и заметно снижает шанс допустить ошибку в обработке пользовательского ввода.

Кроме этого, argparse полезен в архитектуре, где параметры можно задавать не
только для одной команды, но и как составные части — например, с флагами,
которые активируют разные режимы работы. Учитывая, что основа инструмента
строится вокруг запуска изолированных окружений с множеством вариантов — таких
как  приватный том, оверлей, сохранение состояния, удаление и т. д. — без
чёткого описания структуры командной строки всё быстро деградировало бы в сложно
поддерживаемую кодовую базу с множеством условных операторов.

### shlex
shlex, в отличие от argparse, работает на другом уровне — он парсит строки в
формате shell. Это удобно, если некоторые параметры передаются как готовый кусок
командной строки, который нужно разложить на отдельные аргументы, чтобы
корректно обработать дополнительные флаги, которые нужно просто передать дальше
без вмешательства. Потому что независимо от того, насколько много сценариев
использования можно предусмотреть, не стоит ограничивать пользователя, который
умеет работать с linux namespaces, от более точной настройки окружения.

### logging
Отладка инструментов, работающих с пространствами имён, монтированием, правами и
запуском других процессов — задача не тривиальная. Когда поведение процесса
зависит не только от входных аргументов, но и от состояния окружения, порядка
вызовов и других малозначительных деталей, вывод на stdout или простые print()
быстро перестают быть удобными. Для таких случаев и существует logging.

Он позволяет управлять уровнем сообщений: во время разработки можно включить
подробный отладочный режим, а в стабильной сборке — оставить только
предупреждения и ошибки. Всё это без переписывания кода. Вместо того чтобы раз
за разом закомментировать строки с отладочным выводом, достаточно поменять
уровень логирования — и инструмент становится тише.

Кроме того, можно точно настроить формат сообщений: добавить время, имя файла,
строку, уровень важности. Это особенно полезно, если лог пишется в файл или
используется в CI. В случае ошибок будет видно не только что пошло не так, но и
где именно, и при каких условиях.

### sys, os, shutil, Path
Работа с окружением на низком уровне редко обходится без стандартных библиотек
sys, os, shutil и pathlib. Все они решают разные задачи, и каждая из них дает
удобный доступ к какой-то стороне взаимодействия с операционной системой.

sys — один из самых базовых модулей. Он нужен для работы с аргументами командной
строки, завершения программы с нужным кодом или, например, перенаправления
вывода ошибок. Никакой логики, только прямое взаимодействие с механизмами,
которые предоставляет сам интерпретатор.

os — это удобная обертка над API операционной системы. Работа с переменными
окружения, запуск внешних процессов, создание директорий, установка прав — всё
это делается через эту библиотеку. Именно os даёт доступ к системным вызовам,
которые не обернуты в более высокоуровневые модули.

shutil предоставляет функции для высокоуровневой работы с файлами и
директориями, которые значительно упрощают нетривиальные операции, которые
выходят за рамки базовых возможностей os, такие как рекурсивное удаление,
копирование и перемещение.

Наконец, Path из модуля pathlib — современная альтернатива привычной работе со
строковыми путями. Вместо склеивания с os.path.join или разборов вручную, Path
позволяет манипулировать файловой системой как с объектами. Путь становится не
просто строкой, а полноценным элементом логики. Это упрощает код, делает его
читаемее и меньше подверженным ошибкам. Особенно, если путь нужно не просто
напечатать, а проверить, создать или что-то в нём найти.


# Публикация
Когда инструмент начинает работать стабильно, его нужно подготовить к
использованию вне локальной среды. Это значит не только обеспечить установку, но
и сделать ее повторяемой: зависимости должны устанавливаться корректно, команды
выполняться одинаково, а окружение не требовать ручной доводки. Даже если
утилита рассчитана на узкий круг задач, от того, как она оформлена и
распространяется, зависит, насколько быстро ее смогут попробовать другие.

Формат распространения выбирается исходя из того, кто и как будет использовать
результат. Иногда достаточно обычного архива с бинарником. В других случаях
удобнее упаковать всё в Python-пакет и выложить в PyPI. А если требуется
зафиксировать окружение и исключить лишние зависимости, подойдёт сборка в
контейнер. Но, к сожалению, последний самый контролируемый вариант для данной
утилиты совершенно не подходит.

## PyPI
PyPI (Python Package Index) — это центральное хранилище для Python-библиотек и
утилит, своего рода каталог, откуда любой может установить пакет с помощью pip.
Это упрощает распространение: достаточно один раз загрузить проект, и он
становится доступен через стандартные инструменты во всех системах, где
установлен Python.

У PyPI есть важное преимущество — его используют по умолчанию. Поэтому если цель
— сделать инструмент доступным другим разработчикам, особенно в экосистеме
Python, публикация туда выглядит естественным выбором. Она не требует настройки
стороннего хранилища, и сам процесс сводится к нескольким командам. Всё, что
нужно — правильно оформить проект: указать метаинформацию, зависимости и путь к
исполняемому скрипту.

Это не единственный способ доставки, но один из самых прямых, когда речь идет о
Python-утилите, которую планируется запускать из командной строки.

### Конфигурация пакета
Чтобы опубликовать пакет на PyPI, нужно выполнить несколько базовых требований,
и первое из них — подготовить корректную структуру проекта и метаданные. PyPI
ожидает, что у проекта будет файл pyproject.toml, в котором указано, как его
собирать и что он из себя представляет.

pyproject.toml постепенно вытеснил setup.py и стал основным способом описания
Python-проектов. Причина довольно понятная — со временем в setup.py начинала
появляться не только метаинформация, но и логика, и это мешало его использовать
как универсальный формат. Требовалось что-то более структурированное, что можно
было бы легко читать и обрабатывать автоматически.

Новый формат решает это за счёт разделения обязанностей: вся информация о
проекте — его имя, версия, авторы, список зависимостей — теперь явно задаётся в
одном месте, без смешивания с кодом. При этом pyproject.toml может
использоваться не только setuptools, но и другими инструментами — flit, poetry,
uv, и это делает экосистему более гибкой.

Листинг # - Минимальная конфигурация проекта.

```
[project]
name = "wrapify"
version = "0.1.0"
description = "Lightweight tool for safely running untrusted code in a containerized environment using Bubblewrap"
readme = "README.md"
requires-python = ">=3.13"
dependencies = []
```

Этого достаточно, чтобы pip и другие аналогичные инструменты понимали, как
собрать и установить пакет. Если скрипт предполагается запускать из терминала, в
pyproject.toml можно также указать точку входа — команду, которая будет
устанавливаться как исполняемая, что в данном случае очень удобно:

Листинг # - Указание точки входа.

```
[project.scripts]
wrapify = "main:main"
```

### Сборка и загрузка пакета
Для публикации на PyPI одного pyproject.toml недостаточно. Помимо описания
проекта, нужен специальным образом подготовленный образ — это не просто архив с
исходниками, а строго оформленный пакет, который должен соответствовать
требованиям Python Packaging Authority. Он представляет собой либо sdist (source
distribution), либо wheel (формат .whl), а чаще — оба варианта одновременно. Это
делается для того, чтобы охватить как можно больше сценариев установки:
исходники можно собрать под специфическую платформу, а *колесико* (wheel)
ставится сразу, если всё совпадает по окружению.

Формат sdist — это tar.gz-архив с исходным кодом и всей метаинформацией о
пакете. Он дает максимальную гибкость, но требует компиляции на стороне
пользователя при установке, что очевидно, занимает дополнительное время и
требует некоторых вычислительных ресурсов целевой системы. В отличие от него,
wheel — это бинарный формат: предкомпилированный и готовый к установке. Если
пакет не зависит от платформы (например, написан полностью на Python), то wheel
будет универсальным — py3-none-any.  
Раньше для процесса публикации приходилось использовать несколько отдельных
инструментов: один для сборки (setuptools, flit, hatch), другой для загрузки
(twine). С недавнего времени всё же это может заменить uv — универсальный
инструмент от команды astral.sh, который упрощает цепочку сборки и публикации.
Он берёт на себя сразу несколько ролей: управляет зависимостями, собирает проект
и публикует его. uv стремится быть не только быстрее, но и проще: меньше внешних
конфигураций, меньше подводных камней, больше предсказуемости. Он работает
напрямую с pyproject.toml и подходит для тех случаев, когда не требуется тонкая
настройка процесса, а нужно просто получить работающее решение без лишних
промежуточных шагов.
